# üéì Tutorial: Building Enterprise-Quality Software with TDD and AI

**A Step-by-Step Guide for Students**

---

## üìö Table of Contents

1. [Introduction](#introduction)
2. [What We Built](#what-we-built)
3. [The TDD Process We Followed](#the-tdd-process-we-followed)
4. [Step-by-Step Tutorial](#step-by-step-tutorial)
5. [How to Use AI Effectively](#how-to-use-ai-effectively)
6. [Key Lessons Learned](#key-lessons-learned)
7. [Your Turn: Practice Exercise](#your-turn-practice-exercise)

---

## Introduction

This tutorial teaches you how to build **enterprise-quality software** using **Test-Driven Development (TDD)** with the help of **AI assistants** like GitHub Copilot.

### What You'll Learn

‚úÖ How to plan a software project with architecture documents  
‚úÖ How to write tests BEFORE writing code (TDD)  
‚úÖ How to use AI to accelerate development  
‚úÖ How to achieve 90%+ test coverage  
‚úÖ How to structure code for maintainability  
‚úÖ How to create production-ready applications  

---

## What We Built

A **Web Search Demo** that uses OpenAI's API to search the web in real-time with citations.

### Final Results

- **318 lines** of production code
- **69 tests** - all passing ‚úÖ
- **90.25% test coverage**
- **Complete documentation**
- **Clean architecture** with 5 modular components
- **Working CLI application**

### Project Structure

```
demo/
‚îú‚îÄ‚îÄ src/                    # Production code
‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Data structures
‚îÇ   ‚îú‚îÄ‚îÄ client.py          # API client
‚îÇ   ‚îú‚îÄ‚îÄ parser.py          # Response parser
‚îÇ   ‚îú‚îÄ‚îÄ search_service.py  # Business logic
‚îÇ   ‚îî‚îÄ‚îÄ main.py            # CLI application
‚îú‚îÄ‚îÄ tests/                  # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py        # Shared fixtures
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py     # Model tests
‚îÇ   ‚îú‚îÄ‚îÄ test_client.py     # Client tests
‚îÇ   ‚îú‚îÄ‚îÄ test_parser.py     # Parser tests
‚îÇ   ‚îú‚îÄ‚îÄ test_search_service.py  # Service tests
‚îÇ   ‚îî‚îÄ‚îÄ test_main.py       # Integration tests
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ architecture.md    # System design
‚îú‚îÄ‚îÄ pytest.ini             # Test configuration
‚îî‚îÄ‚îÄ requirements.txt       # Dependencies
```

---

## The TDD Process We Followed

### Traditional Development (DON'T DO THIS)

```
1. Write code
2. Test it manually
3. Find bugs
4. Fix bugs
5. Repeat
‚ùå Low confidence, hard to maintain
```

### Test-Driven Development (DO THIS)

```
1. Write a failing test (RED) üî¥
2. Write minimal code to pass (GREEN) üü¢
3. Refactor for quality (REFACTOR) ‚ôªÔ∏è
4. Repeat
‚úÖ High confidence, easy to maintain
```

### Why TDD with AI?

- **AI helps you write tests faster** - Describe what you want, AI generates test cases
- **Tests guide AI** - AI knows exactly what to implement
- **Catch errors early** - Tests verify AI-generated code is correct
- **Refactor safely** - Tests ensure changes don't break functionality

---

## Step-by-Step Tutorial

### Phase 1: Planning (Before Any Code)

#### Step 1.1: Understand the Problem

**What we did:**
```
Goal: Demo OpenAI's web search capability
Requirements:
- Search the web in real-time
- Extract citations
- Display formatted results
- Handle errors gracefully
```

**AI Prompt Used:**
```
"Read this OpenAI web search documentation and give me ideas 
for a simple demo that shows the power of this tool"
```

#### Step 1.2: Create Architecture Document

**What we did:**
- Designed system components
- Defined data models
- Planned testing strategy
- Documented error handling

**AI Prompt Used:**
```
"Create an architecture document that will use test-driven 
development to ensure enterprise quality code. Include:
- System architecture
- Component breakdown
- Data models
- TDD approach
- Testing strategy"
```

**Key Output:** `docs/architecture.md` (400+ lines)

#### Step 1.3: Set Up Testing Infrastructure

**What we did:**
```bash
# Created pytest.ini with configuration
# Set up conftest.py with fixtures
# Created mock data in fixtures/
```

**AI Prompt Used:**
```
"Configure pytest.ini with coverage, test markers, and 
enterprise-quality settings"
```

---

### Phase 2: Test-First Development (The TDD Cycle)

#### Cycle 1: Data Models

##### üî¥ RED - Write Failing Tests First

**File:** `tests/test_models.py`

```python
# We wrote this FIRST, before models.py existed
def test_create_citation():
    """Test creating a Citation with all required fields."""
    citation = Citation(
        url="https://example.com",
        title="Example Article",
        start_index=100,
        end_index=200
    )
    
    assert citation.url == "https://example.com"
    assert citation.title == "Example Article"
    assert citation.start_index == 100
    assert citation.end_index == 200
```

**AI Prompt Used:**
```
"Write comprehensive unit tests for these data models:
- SearchOptions (with defaults)
- Citation (with properties)
- Source (with type checking)
- SearchResult (with citations)
- SearchError (custom exception)
Use pytest and follow AAA pattern (Arrange, Act, Assert)"
```

**Result:** 16 failing tests ‚ùå

##### üü¢ GREEN - Make Tests Pass

**File:** `src/models.py`

```python
# We wrote this SECOND, to make tests pass
@dataclass
class Citation:
    """Represents a citation from a web search result."""
    
    url: str
    title: str
    start_index: int
    end_index: int
    
    @property
    def length(self) -> int:
        """Calculate the length of the cited text."""
        return self.end_index - self.start_index
```

**AI Prompt Used:**
```
"Implement the data models to make these tests pass. Use:
- Python dataclasses
- Type hints
- Properties where needed
- Clean, simple design"
```

**Result:** 16 passing tests ‚úÖ

##### ‚ôªÔ∏è REFACTOR - Improve Code Quality

- Added docstrings
- Added `__str__` methods
- Added helpful properties
- Cleaned up imports

---

#### Cycle 2: Response Parser

##### üî¥ RED - Write Failing Tests

**File:** `tests/test_parser.py`

```python
def test_parse_valid_response(valid_api_response, sample_query):
    """Test parsing a valid API response."""
    parser = ResponseParser()
    result = parser.parse(valid_api_response, sample_query)
    
    assert isinstance(result, SearchResult)
    assert result.query == sample_query
    assert len(result.text) > 0
    assert len(result.citations) > 0
```

**AI Prompt Used:**
```
"Write tests for ResponseParser that:
- Parses valid responses
- Extracts text content
- Extracts citations with URLs and titles
- Extracts sources
- Handles empty responses
- Handles malformed data
- Formats output for display"
```

**Result:** 13 failing tests ‚ùå

##### üü¢ GREEN - Implement Parser

**File:** `src/parser.py`

```python
class ResponseParser:
    def parse(self, response: Dict[str, Any], query: str) -> SearchResult:
        if not response.get("output"):
            raise ValueError("No output in response")
        
        # Extract text and citations...
        return SearchResult(...)
```

**AI Prompt Used:**
```
"Implement ResponseParser to make all tests pass.
Extract: text, citations, sources, search_id
Handle: missing data, malformed responses"
```

**Result:** 13 passing tests ‚úÖ

---

#### Cycle 3: API Client

##### üî¥ RED - Write Tests with Mocks

**File:** `tests/test_client.py`

```python
@patch('src.client.OpenAI')
def test_search_with_basic_query(mock_openai_class, test_api_key, 
                                sample_query, mock_response_object):
    """Test performing a basic web search."""
    # Setup mock
    mock_client_instance = MagicMock()
    mock_client_instance.responses.create.return_value = mock_response_object
    mock_openai_class.return_value = mock_client_instance
    
    # Test
    client = WebSearchClient(api_key=test_api_key)
    response = client.search(sample_query)
    
    # Verify
    assert response is not None
    mock_client_instance.responses.create.assert_called_once()
```

**Key Concept:** **Mock external APIs** - Never call real APIs in tests!

**AI Prompt Used:**
```
"Write tests for WebSearchClient with mocked OpenAI client:
- Test API key validation
- Test successful searches
- Test with custom options
- Test domain filtering
- Test error handling (401, 429, 500)
- Test query validation
Use unittest.mock for mocking"
```

**Result:** 13 failing tests ‚ùå

##### üü¢ GREEN - Implement Client

**AI Prompt Used:**
```
"Implement WebSearchClient to make tests pass:
- Initialize OpenAI client
- Construct API requests
- Handle responses
- Convert to dictionaries
- Handle all error types
- Validate inputs"
```

**Result:** 13 passing tests ‚úÖ

---

#### Cycle 4: Service Layer

##### üî¥ RED - Write Business Logic Tests

**File:** `tests/test_search_service.py`

```python
@patch('src.search_service.WebSearchClient')
@patch('src.search_service.ResponseParser')
def test_search_success(mock_parser_class, mock_client_class, 
                       test_api_key, sample_query):
    """Test successful search operation."""
    # Setup mocks
    mock_client = MagicMock()
    mock_client.search.return_value = valid_api_response
    
    mock_parser = MagicMock()
    mock_parser.parse.return_value = expected_result
    
    # Test
    service = SearchService(api_key=test_api_key)
    result = service.search(sample_query)
    
    # Verify
    assert result is not None
    mock_client.search.assert_called_once()
    mock_parser.parse.assert_called_once()
```

**AI Prompt Used:**
```
"Write tests for SearchService business logic:
- Test initialization
- Test successful searches
- Test query validation
- Test error handling from client
- Test error handling from parser
- Test domain filter application
Use mocks for client and parser"
```

##### üü¢ GREEN - Implement Service

**AI Prompt Used:**
```
"Implement SearchService to orchestrate:
- Validate queries
- Call client
- Parse responses
- Handle errors
- Apply business rules"
```

---

#### Cycle 5: CLI Application

##### üî¥ RED - Integration Tests

**File:** `tests/test_main.py`

```python
@patch('src.main.SearchService')
@patch.dict('os.environ', {'OPENAI_API_KEY': 'test-key'})
def test_main_success_flow(mock_service_class):
    """Test successful main execution flow."""
    mock_service = MagicMock()
    mock_service.search.return_value = result
    
    test_args = ["prog", "AI news"]
    with patch.object(sys, 'argv', test_args):
        exit_code = main()
    
    assert exit_code == 0
```

**AI Prompt Used:**
```
"Write integration tests for main CLI:
- Test argument parsing
- Test display functions
- Test end-to-end flow
- Test error handling
- Test without API key
Mock SearchService, not external APIs"
```

##### üü¢ GREEN - Implement CLI

**AI Prompt Used:**
```
"Implement CLI application with:
- argparse for arguments
- Environment variable support
- Pretty formatted output
- Error handling
- Exit codes"
```

---

### Phase 3: Verification & Documentation

#### Step 3.1: Run Complete Test Suite

```bash
pytest -v --cov=src --cov-report=html
```

**Result:**
```
69 passed ‚úÖ
90.25% coverage ‚úÖ
```

#### Step 3.2: Fix Any Issues

When tests revealed the API response structure was different:

```python
# Before (caused errors)
"created": response.created

# After (works with real API)
"created": getattr(response, 'created', 0)
```

**Lesson:** Tests catch integration issues early!

#### Step 3.3: Create Documentation

- `README.md` - User guide
- `QUICKSTART.md` - Quick start
- `PROJECT_SUMMARY.md` - Overview
- This tutorial!

---

## How to Use AI Effectively

### ‚úÖ DO: Be Specific and Structured

**Good Prompt:**
```
"Create pytest unit tests for a SearchOptions dataclass with:
- model: str = 'gpt-4o-mini'
- allowed_domains: Optional[List[str]] = None
- user_location: Optional[dict] = None

Tests should verify:
1. Default values
2. Custom values
3. Field mutability

Use pytest markers @pytest.mark.unit"
```

**Why it works:**
- Clear structure
- Specific requirements
- Expected behavior defined
- Technical details included

### ‚ùå DON'T: Be Vague

**Bad Prompt:**
```
"Write some tests for options"
```

**Why it fails:**
- No context
- No requirements
- AI must guess

---

### ‚úÖ DO: Provide Context

**Good Prompt:**
```
"I'm building a web search demo using OpenAI's Responses API.
I need to parse the API response which has this structure:
{
  'output': [
    {'type': 'web_search_call', ...},
    {'type': 'message', 'content': [...]}
  ]
}

Write a ResponseParser class that extracts:
- Text from message content
- Citations from annotations
- Sources from search action

Include error handling for missing fields."
```

**Why it works:**
- Project context provided
- Data structure shown
- Requirements clear
- Edge cases mentioned

---

### ‚úÖ DO: Iterate and Refine

**Conversation Flow:**

1. **You:** "Write tests for a parser"
2. **AI:** [Generates basic tests]
3. **You:** "Add tests for error cases: empty response, malformed data"
4. **AI:** [Adds error tests]
5. **You:** "Add tests for edge case where citations array is missing"
6. **AI:** [Adds edge case tests]

**Lesson:** Build incrementally, don't expect perfection first try

---

### ‚úÖ DO: Verify AI Output

**After AI generates code:**

1. **Read it** - Understand what it does
2. **Run tests** - Verify it works
3. **Check coverage** - Ensure completeness
4. **Review logic** - Make sure it makes sense
5. **Refactor** - Improve if needed

**Never blindly accept AI code!**

---

### ‚úÖ DO: Use AI for Boilerplate

**Great AI Uses:**
- Test fixtures and mock data
- Docstrings and comments
- Type hints
- Error handling patterns
- Configuration files

**Example Prompt:**
```
"Generate pytest fixtures for:
- Mock OpenAI client
- Sample API response (valid)
- Sample API response (error)
- Test configuration
Store in conftest.py"
```

---

### ‚úÖ DO: Follow TDD Order

**The Process:**

1. **Describe what you want** (requirements)
2. **Ask AI to write tests first** 
3. **Run tests** (they should fail)
4. **Ask AI to implement** to pass tests
5. **Run tests again** (they should pass)
6. **Review and refactor**

**Example:**
```
You: "Write tests for a function that validates email addresses"
AI: [Generates tests]
You: "Now implement the email validator to pass these tests"
AI: [Generates implementation]
```

---

## Key Lessons Learned

### 1. **Architecture First**

‚ùå **Wrong:** Start coding immediately  
‚úÖ **Right:** Plan architecture, then code

**Why:** 
- Clear roadmap prevents confusion
- Components fit together cleanly
- Easy to divide work

### 2. **Tests Before Code**

‚ùå **Wrong:** Write code, then test  
‚úÖ **Right:** Write tests, then code to pass them

**Why:**
- Tests define requirements clearly
- Higher confidence in code
- Easier to refactor later

### 3. **Small Increments**

‚ùå **Wrong:** Build entire system at once  
‚úÖ **Right:** Build one component at a time

**Why:**
- Easier to debug
- Can test each piece
- Less overwhelming

### 4. **Mock External Dependencies**

‚ùå **Wrong:** Call real APIs in tests  
‚úÖ **Right:** Mock external services

**Why:**
- Tests run fast
- No API costs
- Tests don't fail from network issues

### 5. **High Coverage ‚â† Good Tests**

‚ùå **Wrong:** Focus only on coverage %  
‚úÖ **Right:** Write meaningful tests

**Why:**
- Coverage shows what's tested, not quality
- Edge cases matter more than coverage number
- Good tests test behavior, not implementation

### 6. **AI is a Tool, Not Magic**

‚ùå **Wrong:** Let AI do everything without review  
‚úÖ **Right:** Use AI to accelerate, but verify everything

**Why:**
- AI can make mistakes
- AI doesn't understand business logic perfectly
- You're responsible for the code

### 7. **Documentation Matters**

‚ùå **Wrong:** Skip documentation, "code explains itself"  
‚úÖ **Right:** Document architecture, decisions, and usage

**Why:**
- Future you will thank present you
- Others can understand the system
- Makes maintenance easier

---

## Your Turn: Practice Exercise

### üéØ Challenge: Build a Weather Dashboard

Apply everything you learned to build a weather dashboard using TDD!

#### Requirements

**Features:**
- Fetch current weather for a city (use OpenWeatherMap API)
- Display temperature, conditions, humidity
- Support multiple cities
- Cache results for 5 minutes
- Handle errors gracefully

**Architecture:**
- `models.py` - Weather, City, ForecastData
- `client.py` - API client for OpenWeatherMap
- `cache.py` - Simple caching layer
- `weather_service.py` - Business logic
- `main.py` - CLI or web interface

#### Step-by-Step Process

##### 1. **Planning (30 minutes)**

Create `architecture.md`:
```markdown
# Weather Dashboard Architecture

## Components
- WeatherClient: API calls
- CacheService: Store results
- WeatherService: Business logic
- CLI: User interface

## Data Models
- City: name, country, coordinates
- Weather: temp, condition, humidity
- Cache: timestamp, data

## Testing Strategy
- Mock API calls
- Test cache expiration
- Test error handling
```

**AI Prompt:**
```
"Help me design a weather dashboard architecture using:
- OpenWeatherMap API
- Caching (5 min TTL)
- CLI interface
- TDD approach

Include component breakdown and data models"
```

##### 2. **Setup (15 minutes)**

```bash
# Create project structure
mkdir weather_dashboard
cd weather_dashboard
mkdir src tests docs

# Create files
touch src/{__init__,models,client,cache,weather_service,main}.py
touch tests/{conftest,test_models,test_client,test_cache,test_service,test_main}.py
touch pytest.ini requirements.txt
```

**AI Prompt:**
```
"Generate pytest.ini configuration for a weather dashboard project
with coverage reporting and test markers"
```

##### 3. **TDD Cycle 1: Models (45 minutes)**

**Write tests first:**
```python
# tests/test_models.py

def test_create_weather():
    """Test creating a Weather object."""
    weather = Weather(
        temperature=72.5,
        condition="Sunny",
        humidity=45
    )
    
    assert weather.temperature == 72.5
    assert weather.condition == "Sunny"
    assert weather.humidity == 45

def test_weather_temperature_in_celsius():
    """Test converting Fahrenheit to Celsius."""
    weather = Weather(temperature=68.0, ...)
    assert weather.temp_celsius == 20.0
```

**AI Prompt:**
```
"Write pytest tests for these weather data models:
- City (name, country, lat, lon)
- Weather (temp, condition, humidity, timestamp)
- Forecast (list of weather, city)

Include tests for:
- Object creation
- Temperature conversion (F to C)
- Timestamp handling
- Edge cases"
```

**Then implement:**

**AI Prompt:**
```
"Implement the data models to pass these tests.
Use dataclasses, type hints, and properties."
```

##### 4. **TDD Cycle 2: API Client (60 minutes)**

**Write tests with mocks:**

**AI Prompt:**
```
"Write tests for WeatherClient that calls OpenWeatherMap API:
- Mock requests library
- Test successful API call
- Test API errors (401, 404, 500)
- Test timeout handling
- Test response parsing

Never call real API in tests!"
```

**Then implement:**

**AI Prompt:**
```
"Implement WeatherClient to pass tests:
- Use requests library
- Handle all error cases
- Parse JSON response
- Add retry logic"
```

##### 5. **TDD Cycle 3: Cache (45 minutes)**

**Write tests:**

**AI Prompt:**
```
"Write tests for a simple cache with 5-minute TTL:
- Test storing data
- Test retrieving data
- Test cache expiration
- Test cache miss
Use freezegun for time manipulation"
```

**Implement:**

**AI Prompt:**
```
"Implement simple in-memory cache with TTL support"
```

##### 6. **Continue Pattern for Service & CLI**

Follow the same TDD cycle for:
- `weather_service.py` (business logic)
- `main.py` (CLI interface)

##### 7. **Verification**

```bash
# Run tests
pytest -v --cov=src --cov-report=html

# Target: 90%+ coverage
```

##### 8. **Documentation**

Create:
- `README.md` with usage instructions
- API documentation
- Architecture diagram

**AI Prompt:**
```
"Generate a README for a weather dashboard with:
- Installation instructions
- Usage examples
- API key setup
- Screenshots (describe them)"
```

---

### üèÜ Success Criteria

Your weather dashboard should have:

- ‚úÖ **Architecture document** planned before coding
- ‚úÖ **Tests written first** for every component
- ‚úÖ **90%+ test coverage**
- ‚úÖ **All tests passing**
- ‚úÖ **Clean code** with type hints and docstrings
- ‚úÖ **Error handling** for all edge cases
- ‚úÖ **Mocked external APIs** in tests
- ‚úÖ **Working CLI application**
- ‚úÖ **Complete documentation**

---

## Prompts Cheat Sheet

### For Architecture

```
"Create a software architecture document for [PROJECT] that includes:
- System components and their responsibilities
- Data models with fields and types
- API integrations needed
- Error handling strategy
- TDD approach
- Testing strategy with unit/integration tests"
```

### For Test Writing

```
"Write pytest unit tests for [COMPONENT] that:
- Test happy path (normal usage)
- Test edge cases (empty, null, boundaries)
- Test error cases (invalid input, exceptions)
- Use fixtures for test data
- Use mocks for external dependencies
- Follow AAA pattern (Arrange, Act, Assert)
Mark with @pytest.mark.unit"
```

### For Implementation

```
"Implement [COMPONENT] to pass these tests:
[paste test code]

Requirements:
- Use type hints
- Add docstrings (Google style)
- Handle errors gracefully
- Follow SOLID principles
- Keep functions small and focused"
```

### For Refactoring

```
"Refactor this code to improve:
- Readability (clear names, simple logic)
- Maintainability (DRY, single responsibility)
- Performance (if applicable)
- Error handling

Maintain all existing tests passing."
```

### For Documentation

```
"Generate documentation for [PROJECT]:
- User guide with examples
- API reference
- Installation instructions
- Troubleshooting section
- Architecture overview"
```

---

## Summary: The TDD+AI Workflow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. PLAN                                    ‚îÇ
‚îÇ     - Architecture document                 ‚îÇ
‚îÇ     - Define components                     ‚îÇ
‚îÇ     - Plan data models                      ‚îÇ
‚îÇ     Use AI: "Design architecture for..."    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. SETUP                                   ‚îÇ
‚îÇ     - Create project structure              ‚îÇ
‚îÇ     - Configure pytest                      ‚îÇ
‚îÇ     - Set up fixtures                       ‚îÇ
‚îÇ     Use AI: "Generate pytest.ini..."        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. RED: Write Failing Tests                ‚îÇ
‚îÇ     - Define expected behavior              ‚îÇ
‚îÇ     - Cover edge cases                      ‚îÇ
‚îÇ     - Use mocks for external deps           ‚îÇ
‚îÇ     Use AI: "Write tests for..."            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. Verify Tests Fail                       ‚îÇ
‚îÇ     - Run: pytest                           ‚îÇ
‚îÇ     - All new tests should fail ‚ùå          ‚îÇ
‚îÇ     - If passing, tests are wrong!          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. GREEN: Make Tests Pass                  ‚îÇ
‚îÇ     - Write minimal code                    ‚îÇ
‚îÇ     - Make tests pass                       ‚îÇ
‚îÇ     - Don't optimize yet                    ‚îÇ
‚îÇ     Use AI: "Implement to pass tests..."    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. Verify Tests Pass                       ‚îÇ
‚îÇ     - Run: pytest                           ‚îÇ
‚îÇ     - All tests should pass ‚úÖ              ‚îÇ
‚îÇ     - Check coverage                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  7. REFACTOR: Improve Quality               ‚îÇ
‚îÇ     - Clean up code                         ‚îÇ
‚îÇ     - Add comments                          ‚îÇ
‚îÇ     - Optimize if needed                    ‚îÇ
‚îÇ     - Tests ensure nothing breaks           ‚îÇ
‚îÇ     Use AI: "Refactor this code..."         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  8. Repeat for Next Component               ‚îÇ
‚îÇ     Go back to step 3                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  9. DOCUMENT                                ‚îÇ
‚îÇ     - Write README                          ‚îÇ
‚îÇ     - API documentation                     ‚îÇ
‚îÇ     - Usage examples                        ‚îÇ
‚îÇ     Use AI: "Generate docs for..."          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Additional Resources

### Books
- "Test Driven Development: By Example" - Kent Beck
- "Clean Code" - Robert Martin
- "The Pragmatic Programmer" - Hunt & Thomas

### Online
- [pytest documentation](https://docs.pytest.org/)
- [Python testing best practices](https://docs.python-guide.org/writing/tests/)
- [GitHub Copilot tips](https://github.blog/developer-skills/github/how-to-use-github-copilot-for-pair-programming/)

### This Project
- See `docs/architecture.md` for our architecture
- Read test files for examples
- Study the commit history (if available)

---

## Final Tips

1. **Start Small** - Build one feature at a time
2. **Test First** - Always write tests before code
3. **Use AI Wisely** - Verify everything it generates
4. **Document Everything** - Your future self will thank you
5. **Refactor Often** - Keep code clean as you go
6. **Ask for Help** - Use AI, colleagues, documentation
7. **Practice** - Do the exercise, build more projects
8. **Learn from Tests** - Tests teach you how code should work

---

## Questions?

If you have questions about this tutorial or TDD in general:

1. Review the code in this project
2. Read the architecture document
3. Try the practice exercise
4. Experiment and learn by doing!

---

**Built with:** Python, pytest, TDD, and AI assistance  
**For:** Students learning enterprise software development  
**By:** Following the exact process described in this tutorial  

üéâ **Now go build something amazing!** üéâ
